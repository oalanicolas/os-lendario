# The Alignment Problem

**Author:** Brian Christian
**Description:** Machine Learning and Human Values
**Duration:** 7 min | **Rating:** 2.9/5

---

## INTRODUCTION: What’s in it for me? Learn how AI programs hold up a mirror to the worst in humanity.

Talk to anyone, and they’ll have an opinion about AI. “It’s going to make work better.” “It’s going to take our jobs.” “It’s going to bring world peace.” “It’s going to destroy the world.” There is still much we don’t know about what AI will evolve into, who will be using it, and how they will be using it. What we do know is that it is developing at a rapid pace and being unleashed on the world without a decent amount of testing. And in this process, it’s teaching us something about ourselves. One of the shocking behaviors frequently exhibited by AI is its bias and racism. To understand why this is, we have to take a trip all the way back to the nineteenth century when Frederick Douglass became the most photographed person in the world. In this very short Blink, we’ll travel all the way up to the early 2010’s, when a young developer found that her AI robot refused to recognize her face unless she wore a white mask. Lastly, we’ll talk about the hopes and warnings that these stories have for us.

---

## CHAPTER 1 OF 2: AI is often racist

In 2015 a young web developer named Jacky Alciné got a notification that his friend had shared a photo with him on Google photos. When he opened up the app, he saw that a new user interface (or UI) had been installed. Now, Google’s AI was grouping the photos into categories like “graduation” or “the beach.” Alciné saw a selfie with him and his best friend, both of whom are black. The caption under the selfie read “gorillas.” When he opened up the folder, he saw dozens of pictures of him and his friend, and nothing else. He immediately went to Twitter and called out Google photos. He received a response within two hours and Google went to work resolving the issue. As of 2023, their best solution was to remove the category of gorillas from their UI. They still haven’t found a way to get their program to identify gorillas without mis-categorizing black people. To understand how this happened, we have to go back to the most photographed person of the nineteenth century – Frederick Douglass. The famous abolitionist was happy when the technology of photography began to be accessible to the public. Until then, the only representations of black people had been in drawings done by white people. These drawings largely exaggerated black people’s features, making them appear more like animals than humans. Douglass believed photography would give black people better representation. He was happy to pose for photos for this reason, and he encouraged black people to take up photography. This was a good start, but the problem wasn’t only representation. The problem went right down to the technology itself. Essentially, cameras themselves were racist. This is because film was created with a special chemical coating, and that coating had been created based on how the person or object being filmed or photographed looked in various lights. To achieve good pictures in film, Hollywood took to using a white woman (the first was named Shirley) to sit for the camera and optimize the film. The coating of the film was developed to make Shirley look her best. This process completely disregarded people with darker skin tones. Fortunately, this issue was resolved by Kodak in the ‘70s – but not because of the civil rights movement; rather, it was because furniture and candy companies wanted better photographic representations of their products in the media. So Kodak began optimizing film to include darker colors. A happy side effect (for Kodak) was that it opened up a whole new demographic. On the downside, decades of photography and film were missing accurate and clear representations of anyone who wasn’t white. Fast forward to Alciné’s time and we see instances of AI not recognizing black people as human – or not recognizing their faces at all. So now that we understand how intertwined racism is with technology, in the next section we’ll talk about why today’s AI can’t overcome this problem – and what is being done about it.

---

## CHAPTER 2 OF 2: How to create inclusive AI

In the early 2010s, a graduate student named Joy Buolamwini was working on a robotics project. The robot she was working with was trained in facial recognition, and she was trying to teach it to play peek-a-boo. The only problem was, it didn’t recognize her face. To complete the project, she had to have a friend stand in for her. A few years later, she was visiting Hong Kong and receiving a tour and demonstration of a social robot. Once again, the robot recognized everyone in her tour group except her. As it turned out, the robot had been programmed in the same open-source code as her peek-a-boo robot. Eventually, Buolamwini identified the problem. The original program was trained on photos from a dataset called Faces in the Wild. When she looked into it, she found that these images were highly skewed toward white males. Less than five percent were images of dark-skinned females. No wonder Buolamwini’s robots didn’t recognize her face. She reached out to several technology firms to share what she’d discovered. The only one who responded positively was IBM. They verified her results and then went to work improving the datasets and retraining the algorithm. Within a few weeks, they’d reduced errors in identifying the faces of Black women by ten times. This story is just the beginning. When considering whether AI is the wave of the future or will destroy us, we have to look deeper into the past than we’re currently doing. Essentially, AI is only as good as the training it gets. When we see new, open-source AI programs behaving badly, it’s likely because they are training on the internet, which is created by people – and people don’t always exhibit the most desirable behavior. The warning here is that developers have made huge mistakes by not understanding the nature of the history and humanity that inform our technology, and rushing to push out new technologies poses a threat to our future.

---

## CONCLUSION: Final summary

We can’t develop AI independent of human history. These programs are only as good as the datasets they train on. America’s history of misrepresenting or excluding Black people from photography and film has paved the way for datasets that are heavily skewed in favor of white males. The only way to correct this specific problem is to change the datasets. However, the larger lesson is that pushing out new AI technologies without testing and diverse perspectives is reckless and potentially harmful to humanity.
